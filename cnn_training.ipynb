{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.youtube.com/watch?v=9OHlgDjaE2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Transforms\n",
    "transformer = transforms.Compose([transforms.Resize((150,150)) # resizes image to 150 x 150 pixels \n",
    "                                  , transforms.RandomHorizontalFlip() # flips images  randomly, creates another copy\n",
    "                                  , transforms.ToTensor() # 0-255 to 0-1, from numpy to tensors\n",
    "                                  , transforms.Normalize([0.5, 0.5, 0.5] # 0-1 to [-1, 1], formual: (x-mean)/stdev\n",
    "                                                         , [0.5, 0.5, 0.5]\n",
    "                                                        )\n",
    "                                 ]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for the training and testing directories\n",
    "train_path = '/home/david/github/intel_image_classification/scene_detection/seg_train/seg_train'\n",
    "test_path = '/home/david/github/intel_image_classification/scene_detection/seg_test/seg_test'\n",
    "\n",
    "# Dataloader, helps to load the data\n",
    "train_loader = DataLoader(torchvision.datasets.ImageFolder(train_path, transform = transformer)\n",
    "                          , batch_size = 64\n",
    "                          , shuffle = True\n",
    "                         )\n",
    "\n",
    "test_loader = DataLoader(torchvision.datasets.ImageFolder(test_path, transform = transformer)\n",
    "                         , batch_size = 32\n",
    "                         , shuffle = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categories\n",
    "root = pathlib.Path(train_path)\n",
    "classes = sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Network class\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes = 6):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Output size after convolution filter\n",
    "        # ((w-f+2P)/s) + 1\n",
    "        # w is 150, f = kernel size filter 3, padding P = 1, stride s = 1\n",
    "        \n",
    "        # Input shape = (256, 3, 150, 150) # 256 is the batch size, 3 is the channels RGB, 150 x 150 width and height\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3\n",
    "                               , out_channels = 12\n",
    "                               , kernel_size = 3\n",
    "                               , stride = 1\n",
    "                               , padding = 1\n",
    "                              )\n",
    "        \n",
    "        # New shape = (256, 12, 150, 150) new depth of 12 not 3\n",
    "        self.bn1 = nn.BatchNorm2d(num_features = 12)\n",
    "        # shape = (256, 12, 150, 150)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # shape = (256, 12, 150, 150)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size = 2) # reduce the image size factor of 2\n",
    "        # shape = (256, 12, 75, 75)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = 12\n",
    "                               , out_channels = 20\n",
    "                               , kernel_size = 3\n",
    "                               , stride = 1\n",
    "                               , padding = 1\n",
    "                              )\n",
    "        # New shape = (256, 20, 75, 75) \n",
    "        self.relu2 = nn.ReLU()\n",
    "        # New shape = (256, 20, 75, 75) \n",
    "        \n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = 20\n",
    "                       , out_channels = 32\n",
    "                       , kernel_size = 3\n",
    "                       , stride = 1\n",
    "                       , padding = 1\n",
    "                      )\n",
    "        # New shape = (256, 32, 75, 75) \n",
    "        self.bn3 = nn.BatchNorm2d(num_features = 32)\n",
    "        # New shape = (256, 32, 75, 75) \n",
    "        self.relu3 = nn.ReLU()\n",
    "        # New shape = (256, 32, 75, 75) \n",
    "        \n",
    "        # Add the fully connected layer\n",
    "        self.fc = nn.Linear(in_features = 32*75*75 \n",
    "                            , out_features = num_classes\n",
    "                           )\n",
    "        \n",
    "        # Feed forward function \n",
    "        \n",
    "        def forward(self, input):\n",
    "            output = self.conv1(input)\n",
    "            output = self.bn1(output)\n",
    "            output = self.relu1(output)\n",
    "            \n",
    "            output = self.pool(output)\n",
    "            \n",
    "            output = self.conv2(output)\n",
    "            output = self.relu2(output)\n",
    "            \n",
    "            output = self.conv3(output)\n",
    "            output = self.bn3(output)\n",
    "            output = self.relu3(output)\n",
    "            \n",
    "            # Above output will be in matrix form, with shape (256, 32, 75, 75)\n",
    "            \n",
    "            output = output.view(-1, 32*75*75)\n",
    "            \n",
    "            output = self.fc(output)\n",
    "            \n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(num_classes = 6).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = Adam(model.parameters()\n",
    "                 , lr=0.001\n",
    "                 , weight_decay = 0.0001\n",
    "                )\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 10 Epochs \n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Count: 14034\n",
      "Test Count: 3000\n"
     ]
    }
   ],
   "source": [
    "# Calculatiing th esize of training and testing images\n",
    "train_count = len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count = len(glob.glob(test_path+'/**/*.jpg'))\n",
    "\n",
    "print(f'Train Count: {train_count}')\n",
    "print(f'Test Count: {test_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-529c3b9472ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training and saving best model\n",
    "\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Evalution oand training on training dataset\n",
    "    model.train()\n",
    "    train_accuracy = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().data*images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        \n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "    \n",
    "    train_accuracy = train_accuracy / train_count\n",
    "    train_loss = train_loss / train_count\n",
    "    \n",
    "    \n",
    "    # Evluation on testing dataset\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = 0.0\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "            \n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        \n",
    "        test_accuracy = test_accuracy / test_count\n",
    "        \n",
    "        print(f'Epoch: {epoch} Train Loss: {train_loss} Train Accuracy: {train_accuracy} Test Accuracy: {test_accuracy}')\n",
    "        \n",
    "        # Save the best model\n",
    "        \n",
    "        if test_accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "            best_accuracy = test_accuracy\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
